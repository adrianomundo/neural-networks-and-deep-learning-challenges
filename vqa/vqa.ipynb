{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments about Assignment 3\n",
    "#### Adriano Mundo 10524163 , Mario Sacaj 10521887\n",
    "\n",
    "Assignment 3 for the ANN2DL course was to participate a competition on kaggle with the objective to develop a Neural Network Architecture for the Visual Answering Question problem.\n",
    "\n",
    "In order to solve the VQA problem, we searched the architecture that best fit our problem. We studied the Relation Network. \n",
    "\n",
    "Then, we first tried to implement a simple Relation Network by ourselves, just to try to understand how it works. \n",
    "We adapted the CNN structure of the first assignment and implemented a Recurrent Neural network taking into account the one from the third lab. \n",
    "Therefore, it performed very poorly. We then looked for inspiration on the Internet and found this implementation: \n",
    "\n",
    "https://github.com/moduIo/Relation-Networks\n",
    "\n",
    "We adapted it to our domain problem and tweaked it, namely the epochs, the batchsize and the steps per epoch. \n",
    "We tried also other tweaks, like the learning rate, the optimizer, the number of neurons and the drop out parameter. We also tried to get rid of some layers of the model structure so to simplify it but didn't work as expected. \n",
    "This notebook containts the parameter that achieved the best result on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set 'True' if you want to commit without retraining the model\n",
    "commit = False\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "VALIDATION_SPLIT = 0.15\n",
    "\n",
    "# Set Dataset dir\n",
    "dataset_dir = '/kaggle/input/ann-and-dl-vqa/dataset_vqa'\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import json\n",
    "import os.path\n",
    "import random as ra\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Lambda, Embedding, LSTM, Conv2D, MaxPooling2D, TimeDistributed, RepeatVector, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from scipy import ndimage, misc\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 7000\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = .00025\n",
    "vocab_size = 1024\n",
    "sequence_length = 64\n",
    "img_rows, img_cols = 320, 480\n",
    "image_input_shape = (img_rows, img_cols, 3)\n",
    "num_labels = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "def process_image(x):\n",
    "    target_height, target_width = 128, 128\n",
    "    rotation_range = .05 # In radians\n",
    "    degs = ra.uniform(-rotation_range, rotation_range)\n",
    "\n",
    "    x = tf.image.resize(x, (target_height, target_width), method=tf.image.ResizeMethod.AREA)\n",
    "    #x = tfa.image.rotate(x, degs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relations\n",
    "def get_relation_vectors(x):\n",
    "    objects = []\n",
    "    relations = []\n",
    "    shape = K.int_shape(x)\n",
    "    k = 25     # Hyperparameter which controls how many objects are considered\n",
    "    keys = []\n",
    "\n",
    "    # Get k unique random objects\n",
    "    while k > 0:\n",
    "        i = ra.randint(0, shape[1] - 1)\n",
    "        j = ra.randint(0, shape[2] - 1)\n",
    "\n",
    "        if not (i, j) in keys:\n",
    "            keys.append((i, j))\n",
    "            objects.append(x[:, i, j, :])\n",
    "            k -= 1\n",
    "\n",
    "    # Concatenate each pair of objects to form a relation vector\n",
    "    for i in range(len(objects)):\n",
    "        for j in range(i, len(objects)):\n",
    "            relations.append(K.concatenate([objects[i], objects[j]], axis=1))\n",
    "\n",
    "    # Restack objects into Keras tensor [batch, relation_ID, relation_vectors]\n",
    "    return K.permute_dimensions(K.stack([r for r in relations], axis=0), [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "path = '/kaggle/input/ann-and-dl-vqa/dataset_vqa/'\n",
    "questions_path = path + '/train_data.json'\n",
    "with open(questions_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data['questions']\n",
    "data_train, data_valid= train_test_split(data, test_size=VALIDATION_SPLIT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data train generator\n",
    "def custom_generator(data):\n",
    "    while True:\n",
    "        path = '/kaggle/input/ann-and-dl-vqa/dataset_vqa/'\n",
    "        questions_path = path + '/train_data.json'\n",
    "        images_path = path + '/train/'\n",
    "        tokenize = None\n",
    "        n = batch_size\n",
    "        batch_data = []\n",
    "        x_text = []     # List of questions\n",
    "        x_image = []    # List of images\n",
    "        y = []          # List of answers\n",
    "        num_labels = 0  # Current number of labels, used to create index mapping\n",
    "        labels = {}     # Dictionary mapping of ints to labels\n",
    "        images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "\n",
    "        batch_data.append(random.sample(data, n))\n",
    "\n",
    "        labels= {'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'no': 11, 'yes': 12}\n",
    "\n",
    "        for q in batch_data[0][0:n]:\n",
    "            if not q['image_filename'] in images:\n",
    "                images[q['image_filename']] = imageio.imread(images_path + q['image_filename'], pilmode=\"RGB\")\n",
    "\n",
    "            x_text.append(q['question'])\n",
    "            x_image.append(images[q['image_filename']])\n",
    "            y.append(labels[q['answer']])\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "        tokenizer.fit_on_texts(x_text)\n",
    "        sequences = tokenizer.texts_to_sequences(x_text)\n",
    "        x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "        # Convert x_image to np array\n",
    "        x_image = np.array(x_image)\n",
    "\n",
    "        # Convert labels to categorical labels\n",
    "        y = keras.utils.to_categorical(y, num_labels)\n",
    "        yield ([x_text, x_image], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "callbacks=[]\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "\n",
    "model = Model(inputs=[text_inputs, image_inputs], outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "if not commit:\n",
    "    model.fit(custom_generator(data_train),\n",
    "          epochs=epochs,\n",
    "         steps_per_epoch=len(data_train)//batch_size,\n",
    "         validation_data = custom_generator(data_valid),\n",
    "         callbacks=callbacks,\n",
    "         validation_steps=len(data_valid)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data test generator\n",
    "def load_data_test(n, vocab_size, sequence_length, tokenizer=None):\n",
    "    path = '/kaggle/input/ann-and-dl-vqa/dataset_vqa/'\n",
    "    questions_path = path + '/test_data.json'\n",
    "    images_path = path + '/test/'\n",
    "\n",
    "    x_text = []     # List of questions\n",
    "    x_image = []    # List of images\n",
    "    num_labels = 0  # Current number of labels, used to create index mapping\n",
    "    labels = {}     # Dictionary mapping of ints to labels\n",
    "    images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "    # Attempt to load saved JSON subset of the questions\n",
    "    print('Loading data...')\n",
    "        \n",
    "    with open(questions_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data = data['questions'][0:n]\n",
    "    \n",
    "    for q in data[0:n]:\n",
    "        # Create an index for each image\n",
    "        if not q['image_filename'] in images:\n",
    "            images[q['image_filename']] = imageio.imread(images_path + q['image_filename'], pilmode=\"RGB\")\n",
    "\n",
    "        x_text.append(q['question'])\n",
    "        x_image.append(images[q['image_filename']])\n",
    "        \n",
    "    # Convert question corpus into sequential encoding for LSTM\n",
    "    print('Processing text data...')\n",
    "    if not tokenizer:\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(x_text)\n",
    "    sequences = tokenizer.texts_to_sequences(x_text)\n",
    "    x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "    # Convert x_image to np array\n",
    "    x_image = np.array(x_image)\n",
    "\n",
    "    print('Text: ', x_text.shape)\n",
    "    print('Image: ', x_image.shape)\n",
    "\n",
    "    return ([x_text, x_image]), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "if not commit:\n",
    "    (test), tok = load_data_test(samples, vocab_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and results writeout\n",
    "if not commit:\n",
    "    out_softmax = model.predict(test)\n",
    "    predicted_class = out_softmax.argmax(axis=-1)\n",
    "\n",
    "    d = {}\n",
    "    for i in range(0, 3000):\n",
    "        d[i] = predicted_class[i]\n",
    "\n",
    "    create_csv(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
