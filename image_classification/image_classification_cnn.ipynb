{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments about Assignment 1\n",
    "#### Adriano Mundo 10524163 , Mario Sacaj 10521887\n",
    "Assignment 1 for the ANN2DL course was to participate a competition on kaggle with the objective to develop a Neural Network Architecture for Image Classification.\n",
    "\n",
    "We, as showed us in the practice classes, developed a Convolutional Neural Network for the classification task.\n",
    "\n",
    "We started with the architecture provided by the professor trying to understand what every piece of code do, then we re-elaborate our architecture, and instead of using the class structure, we decided to use the Sequential architecture, so it was easier to test, adding and removing new layers of the network. \n",
    "\n",
    "In order to divide our training set in train and validation we used the ImageDataGenerator combined with the SEED to make the experiment reproducible with a 80/20 split. \n",
    "\n",
    "Then, we started experimenting with data augmentation in order to improve the network. Analysing our image set we noticed that image are larger than what we expect, so we increased the image shape, and because of our limited amounts of data, we played with the ImageDataGenerator to increase our train dataset, for example adding rotation, width and heigh shift range, brightness range, etc. \n",
    "\n",
    "In order to deal with overfitting, between the two Dense Layer we added a Dropout with 0.5 as a parameter.\n",
    "\n",
    "Then we trained our network using relu as activation function for all the network expect the last layer were we used softmax. The metric, as stated by the task was accuracy, while loss function and optimzer are respectively categorical cross entropy and adam. \n",
    "\n",
    "We obtained a 64% of accuracy, we did other experiments and research and we can say that it's possible to reach about a maxium of 70%  accuracy with these kind of architecture trying to tune again the parameter to find the match. \n",
    "\n",
    "We decided to not use Transfer Learning because it is very easy to conquer this task using a pre-trained network, vgg to obtain an accuracy of more or less 100%. We, as reported in the task assignment ,tried to understand the concepts, what does it make sense to do and what does not make sense without thinking about obtaining the best result because it was very easy to implement that kind of architecture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "a2c5150a-d4b6-4d48-bbbd-635188bedf64",
    "_uuid": "2695ed7b-2887-4347-8e47-e86a986dff66"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# this let the experiment to be reproducible\n",
    "SEED = 1234\n",
    "tf.random.set_seed(1234)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#Dataset directory\n",
    "dataset_dir = \"/kaggle/input/ann-and-dl-image-classification/Classification_Dataset\"\n",
    "\n",
    "#Model weights filepath (set to None if you wanna train your model from scratch)\n",
    "WEIGHTS = '/kaggle/input/model4me/model.h5'\n",
    "weights = None\n",
    "\n",
    "#Create Dataset_split.json for competition purposes (set to None if you don't want to create it)\n",
    "dataset_info = None\n",
    "\n",
    "\n",
    "#Definition of results writedown function\n",
    "def create_csv(results, results_dir=''):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch size\n",
    "bs = 8\n",
    "\n",
    "# img shape\n",
    "img_h = 320\n",
    "img_w = 320\n",
    "\n",
    "# ImageDataGenerator\n",
    "# -------------------------------------------------------\n",
    "# data augmentation to improve performances\n",
    "apply_data_augmentation = True\n",
    "\n",
    "# create training/validation ImageDataGenerator object\n",
    "if apply_data_augmentation:\n",
    "    train_data_gen = ImageDataGenerator(rotation_range=20,\n",
    "                                        width_shift_range=20,\n",
    "                                        height_shift_range=20,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=False,\n",
    "                                        brightness_range = [0.5, 1.5],\n",
    "                                        fill_mode='constant',\n",
    "                                        cval=0,\n",
    "                                        rescale=1./255,\n",
    "                                        validation_split = 0.2)\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# create testing ImageDataGenerator object\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "# -------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of classes\n",
    "num_classes = 20\n",
    "decide_class_indices = True\n",
    "if decide_class_indices:\n",
    "    classes = ['owl',                   # 0\n",
    "               'galaxy',                # 1\n",
    "               'lightning',             # 2\n",
    "               'wine-bottle',           # 3\n",
    "               't-shirt',               # 4\n",
    "               'waterfall',             # 5\n",
    "               'sword',                 # 6\n",
    "               'school-bus',            # 7\n",
    "               'calculator',            # 8\n",
    "               'sheet-music',           # 9\n",
    "               'airplanes',             # 10\n",
    "               'lightbulb',             # 11\n",
    "               'skyscraper',            # 12\n",
    "               'mountain-bike',         # 13\n",
    "               'fireworks',             # 14\n",
    "               'computer-monitor',      # 15\n",
    "               'bear',                  # 16\n",
    "               'grand-piano',           # 17\n",
    "               'kangaroo',              # 18\n",
    "               'laptop']                # 19\n",
    "\n",
    "else:\n",
    "    classes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "4241f93d-0b2f-43ab-bea7-317fcf331044",
    "_uuid": "f4119646-ff9c-4a14-ae4f-0f6934cf85e0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bLwi0cirJFFH",
    "outputId": "c4ce0646-529d-4b53-ed6b-f172c93f4e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1247 images belonging to 20 classes.\n",
      "Found 307 images belonging to 20 classes.\n",
      "Found 500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generators\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "train_gen = train_data_gen.flow_from_directory(training_dir,\n",
    "                                               batch_size=bs,\n",
    "                                               color_mode='rgb',\n",
    "                                               classes=classes,\n",
    "                                               target_size=(img_h, img_w),\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               seed=SEED, subset='training')\n",
    "\n",
    "\n",
    "valid_gen = train_data_gen.flow_from_directory(training_dir,\n",
    "                                               color_mode='rgb',\n",
    "                                               batch_size=bs,\n",
    "                                               classes=classes,\n",
    "                                               target_size=(img_h, img_w),\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False,\n",
    "                                               seed=SEED, subset='validation')\n",
    "\n",
    "test_gen = test_data_gen.flow_from_directory(dataset_dir,\n",
    "                                             classes=['test'],\n",
    "                                             color_mode='rgb',\n",
    "                                             target_size=(img_h, img_w),\n",
    "                                             class_mode=None,\n",
    "                                             shuffle=False,\n",
    "                                             seed=SEED)\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "22871b35-bb89-47a5-bd75-3721b1feb546",
    "_uuid": "86cd8200-f115-4414-9cbe-36817c9445e3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R_LDRXDeJGiO",
    "outputId": "c4b428e8-c39f-4e94-b83a-59e922e16c33"
   },
   "outputs": [],
   "source": [
    "# Create Dataset objects\n",
    "# -------------------------------------------------------\n",
    "# training and repeat\n",
    "#train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    " #                                              output_types=(tf.float32, tf.float32),\n",
    "  #                                             output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "#train_dataset = train_dataset.repeat()\n",
    "\n",
    "# validation and repeat\n",
    "#valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
    "                                               #output_types=(tf.float32, tf.float32),\n",
    "                                               #output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "#valid_dataset = valid_dataset.repeat()\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture with Sequential model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=8,kernel_size=(3, 3),strides=(1, 1), padding='same', activation = 'relu', input_shape=(img_h, img_w, 3)))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=16,kernel_size=(3, 3),strides=(1, 1),padding='same', activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3, 3),strides=(1, 1),padding='same', activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(3, 3),strides=(1, 1),padding='same', activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))         \n",
    "model.add(tf.keras.layers.Conv2D(filters=128,kernel_size=(3, 3),strides=(1, 1),padding='same', activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))   \n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units=512, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "d4104431-b1cc-4d78-88f3-21eece2618b1",
    "_uuid": "8311c316-f281-4c39-9a61-48941e711e94",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "4ROFvTI7JHwN",
    "outputId": "b4ca3b37-2915-4415-ddcb-a3e8cb93635e"
   },
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "# -------------------------------------------------------\n",
    "# loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# validation metrics\n",
    "metrics = ['accuracy']\n",
    "# compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional model_weights loading from file\n",
    "if weights is not None:\n",
    "    model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "6a30416b-c9eb-464a-b8cd-2e3e5a43c11f",
    "_uuid": "7178278d-29d4-478c-bfec-b7d3cabefeb6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "4ROFvTI7JHwN",
    "outputId": "b4ca3b37-2915-4415-ddcb-a3e8cb93635e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 38s 245ms/step - loss: 2.9863 - accuracy: 0.0682 - val_loss: 2.9504 - val_accuracy: 0.1010\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 37s 236ms/step - loss: 2.9074 - accuracy: 0.0986 - val_loss: 2.8330 - val_accuracy: 0.1270\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 36s 232ms/step - loss: 2.7857 - accuracy: 0.1443 - val_loss: 2.6472 - val_accuracy: 0.1954\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 36s 234ms/step - loss: 2.6055 - accuracy: 0.1740 - val_loss: 2.5696 - val_accuracy: 0.2117\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 36s 231ms/step - loss: 2.4292 - accuracy: 0.2654 - val_loss: 2.4168 - val_accuracy: 0.2541\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 36s 231ms/step - loss: 2.3218 - accuracy: 0.3031 - val_loss: 2.2454 - val_accuracy: 0.2997\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 36s 232ms/step - loss: 2.1871 - accuracy: 0.3448 - val_loss: 2.1390 - val_accuracy: 0.3844\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 37s 238ms/step - loss: 2.0476 - accuracy: 0.3801 - val_loss: 2.0689 - val_accuracy: 0.3355\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 37s 239ms/step - loss: 2.0563 - accuracy: 0.3817 - val_loss: 1.9326 - val_accuracy: 0.4397\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 39s 250ms/step - loss: 1.8988 - accuracy: 0.4234 - val_loss: 2.0116 - val_accuracy: 0.4104\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 37s 238ms/step - loss: 1.7999 - accuracy: 0.4475 - val_loss: 1.9258 - val_accuracy: 0.4430\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 37s 240ms/step - loss: 1.7072 - accuracy: 0.4868 - val_loss: 1.9048 - val_accuracy: 0.4691\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 37s 236ms/step - loss: 1.6366 - accuracy: 0.5076 - val_loss: 1.7330 - val_accuracy: 0.4658\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 37s 235ms/step - loss: 1.6346 - accuracy: 0.5052 - val_loss: 1.8587 - val_accuracy: 0.4951\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 37s 236ms/step - loss: 1.5513 - accuracy: 0.5204 - val_loss: 1.8200 - val_accuracy: 0.4919\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 36s 231ms/step - loss: 1.4570 - accuracy: 0.5726 - val_loss: 1.7669 - val_accuracy: 0.5016\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 37s 235ms/step - loss: 1.4244 - accuracy: 0.5533 - val_loss: 1.7071 - val_accuracy: 0.5016\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 37s 238ms/step - loss: 1.4216 - accuracy: 0.5758 - val_loss: 1.6541 - val_accuracy: 0.5081\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 37s 235ms/step - loss: 1.3487 - accuracy: 0.5974 - val_loss: 1.6094 - val_accuracy: 0.5244\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 37s 236ms/step - loss: 1.3382 - accuracy: 0.5734 - val_loss: 1.6523 - val_accuracy: 0.5342\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 37s 240ms/step - loss: 1.2653 - accuracy: 0.6022 - val_loss: 1.6701 - val_accuracy: 0.5244\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 38s 242ms/step - loss: 1.1953 - accuracy: 0.6263 - val_loss: 1.4821 - val_accuracy: 0.5505\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 37s 240ms/step - loss: 1.1571 - accuracy: 0.6351 - val_loss: 1.6205 - val_accuracy: 0.5244\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 37s 239ms/step - loss: 1.1351 - accuracy: 0.6431 - val_loss: 1.6886 - val_accuracy: 0.5472\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 37s 235ms/step - loss: 1.0909 - accuracy: 0.6624 - val_loss: 1.5665 - val_accuracy: 0.5635\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 38s 242ms/step - loss: 1.0619 - accuracy: 0.6696 - val_loss: 1.6713 - val_accuracy: 0.5375\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 37s 235ms/step - loss: 1.0589 - accuracy: 0.6800 - val_loss: 1.6570 - val_accuracy: 0.5309\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if weights is None:\n",
    "    \n",
    "    #Training with callbacks\n",
    "    #-------------------------------------------------------\n",
    "    callbacks = []\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stop = True\n",
    "    if early_stop:\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10) #, restore_best_weights=True)\n",
    "        callbacks.append(es_callback)\n",
    "    model.fit_generator(train_gen, steps_per_epoch=len(train_gen), epochs=100, callbacks=callbacks, validation_data=valid_gen, validation_steps=len(valid_gen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Predictions\n",
    "test_gen.reset()\n",
    "preds = model.predict_generator(test_gen)\n",
    "preds_cls_idx = preds.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down results file\n",
    "image_filenames = test_gen.filenames\n",
    "image_filenames\n",
    "results = {}\n",
    "for i in range(len(image_filenames)):\n",
    "   results[(image_filenames[i][5:])] = preds_cls_idx[i]\n",
    "create_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for the .json file \n",
    "if dataset_info is not None:\n",
    "    from collections import defaultdict\n",
    "    train_dict = defaultdict(list)\n",
    "    for train_image in train_gen.filenames:\n",
    "        clss = train_image.split('/')\n",
    "        key = clss[0]\n",
    "        val = clss[1]\n",
    "        train_dict[key].append(val)\n",
    "\n",
    "    valid_dict = defaultdict(list)\n",
    "    for valid_image in valid_gen.filenames:\n",
    "        clss = valid_image.split('/')\n",
    "        key = clss[0]\n",
    "        val = clss[1]\n",
    "        valid_dict[key].append(val)\n",
    "\n",
    "    master_dict = {}\n",
    "    master_dict['training'] = train_dict\n",
    "    master_dict['validation'] = valid_dict\n",
    "\n",
    "    with open('dataset_split.json', 'w') as json_file:\n",
    "      json.dump(master_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AN2DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
